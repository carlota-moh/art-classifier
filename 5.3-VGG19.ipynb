{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cb118c07",
   "metadata": {},
   "source": [
    "# **5. VGG-19**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ebd7e6-de01-4dd2-a211-099e327856c2",
   "metadata": {
    "id": "3w6rBQM0_o7T"
   },
   "source": [
    "First of all, let's load the required libraries in order to run the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56148681-04e5-4f5c-8865-a7cfaaea8bca",
   "metadata": {
    "id": "Rh87ovI1-TCv"
   },
   "outputs": [],
   "source": [
    "# Base libraries\n",
    "import os\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Tensorflow\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow import debugging as tfdbg\n",
    "from tensorflow import device\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# ResNet50\n",
    "from tensorflow.keras.applications import VGG19\n",
    "\n",
    "# Own modules\n",
    "from src.utils import drop_checkpoints, dataset_stats, plot_metric_curves, extract_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2496f4b5",
   "metadata": {},
   "source": [
    "Now let's see how our data is structured:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "161c96b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root folder\n",
    "base_dir = \"./data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0323057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train folder\n",
    "train_dir = os.path.join(base_dir, \"train\")\n",
    "\n",
    "# Validation folder\n",
    "validation_dir = os.path.join(base_dir, \"validation\")\n",
    "\n",
    "# Test folder\n",
    "test_dir = os.path.join(base_dir, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7676c41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_checkpoints(base_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39825529",
   "metadata": {},
   "source": [
    "Let's also see how many images there are for each class in the training, validation and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c96280e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 4\n",
      "Existing classes: ['Baroque', 'Realism', 'Renaissance', 'Romanticism']\n",
      "\n",
      "----------------------------------------\n",
      "Number of images per class and dataset:\n",
      "----------------------------------------\n",
      "             Train  Validation  Test\n",
      "Style                               \n",
      "Baroque       4000         500   500\n",
      "Realism       4000         500   500\n",
      "Renaissance   4000         500   500\n",
      "Romanticism   4000         500   500\n"
     ]
    }
   ],
   "source": [
    "dataset_stats(train_dir, validation_dir, test_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d922941",
   "metadata": {},
   "source": [
    "We'll also create the directory, if not created yet, where the models will be saved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08160eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory where to save the models created\n",
    "models_dir = \"./models\"\n",
    "os.makedirs(models_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8f8180",
   "metadata": {},
   "source": [
    "*Explain dropout and data augmentation, include reference to original paper*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fe4b50e9-e7b3-41d7-b646-7405621431f2",
   "metadata": {
    "id": "lR3pXpNODl6O",
    "tags": []
   },
   "source": [
    "# **5.1 VGG-19 with frozen convolutional base (just training classifier)**\n",
    "\n",
    "Based on this [paper](https://openaccess.thecvf.com/content_ECCVW_2018/papers/11130/Sabatelli_Deep_Transfer_Learning_for_Art_Classification_Problems_ECCVW_2018_paper.pdf) we found some interesting options for performing transfer learning on our problem. The first one we are going to try is VGG19. Our first approach will be to use this model to extract features from data and then train a fully connected network on top of that. This is the most simple case of transfer learning, where we do not change the base CNN but rather use it for feature extraction (similar to a conventional computer vision approach)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fbba10",
   "metadata": {},
   "source": [
    "## 5.1.1. Model structure\n",
    "\n",
    "Let's first create the model structure:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be97270",
   "metadata": {},
   "source": [
    "Firstly, let's define the values of some hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94fd0f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps per epoch: 125\n"
     ]
    }
   ],
   "source": [
    "# Define some hyperparameters' values\n",
    "\n",
    "# Input shape\n",
    "input_shape = (224, 224,  3)\n",
    "\n",
    "# Batch_size and steps per epoch\n",
    "training_size = sum([len(file) for path, folder, file in os.walk(train_dir)])\n",
    "batch_size = 128\n",
    "steps_per_epoch = training_size // batch_size\n",
    "print(f\"Steps per epoch: {steps_per_epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbdf3f8c-271d-4fea-8a4c-e2ad9137b2a5",
   "metadata": {
    "id": "35S_bt4t-TIX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg19\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv4 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv4 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv4 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,024,384\n",
      "Trainable params: 20,024,384\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "\n",
    "# initialize convolutional base for the model - based onVGG19\n",
    "conv_base = VGG19(weights='imagenet',\n",
    "                  include_top=False,\n",
    "                  pooling=None,\n",
    "                  input_shape=input_shape)\n",
    "\n",
    "conv_base.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "512b42e6",
   "metadata": {},
   "source": [
    "We can now create our DNN classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca883490",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_layer_shape = (7, 7, 512)\n",
    "last_layer_size = last_layer_shape[0]*last_layer_shape[1]*last_layer_shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95db56d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(256, activation='relu', input_dim=last_layer_size))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(4, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb432117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               6422784   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 4)                 516       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,456,196\n",
      "Trainable params: 6,456,196\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07be938-fc08-49cd-9bf3-f2856aa8013b",
   "metadata": {
    "id": "DB3cPq2nLVaR"
   },
   "source": [
    "We'll use Adam as our optimizer since it is the most popular optimizer right now, as well as versatile (i.e., it can be used in multiple contexts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c73ae3a-6296-4aa5-bc43-a9988a33b39c",
   "metadata": {
    "id": "-PG_KXks-TMm"
   },
   "outputs": [],
   "source": [
    "optimizer = optimizers.Adam(learning_rate=1e-4)\n",
    "\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=optimizer,\n",
    "    metrics=['acc']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbb1db3",
   "metadata": {},
   "source": [
    "## 5.1.2. Data preprocessing\n",
    "\n",
    "In this case, we will include the Data Augmentation step to the model preprocessing step..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69134f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply data augmentation to the training set\n",
    "# https://towardsdatascience.com/exploring-image-data-augmentation-with-keras-and-tensorflow-a8162d89b844\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    brightness_range=(0.8, 1),\n",
    "    zoom_range=[0.9, 1.1],\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    "    )\n",
    "\n",
    "# The data augmentation must not be used for the test set!\n",
    "# All images will be rescaled by 1./255\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48c043b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16000 images belonging to 4 classes.\n",
      "Found 2000 images belonging to 4 classes.\n",
      "Found 2000 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        # All images will be resized to the dimensions specified\n",
    "        target_size=input_shape[:2],\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=True\n",
    "        )\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        # All images will be resized to the dimensions specified\n",
    "        target_size=input_shape[:2],\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False\n",
    "        )\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        test_dir,\n",
    "        target_size=input_shape[:2],\n",
    "        batch_size=40,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ba3b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, train_labels = extract_features(16000, last_layer_shape, conv_base, train_generator, batch_size)\n",
    "validation_features, validation_labels = extract_features(2000, last_layer_shape, conv_base, validation_generator, batch_size)\n",
    "test_features, test_labels = extract_features(2000, last_layer_shape, conv_base, test_generator, 40)\n",
    "\n",
    "# flatten features prior to feeding them to the classifier\n",
    "train_features = np.reshape(train_features, (16000, last_layer_size))\n",
    "validation_features = np.reshape(validation_features, (2000, last_layer_size))\n",
    "test_features = np.reshape(test_features, (2000, last_layer_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04597667",
   "metadata": {},
   "source": [
    "Now let's take a look at the output of one of these generators (for instance, the training one):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e735994",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_batch, labels_batch in train_generator:\n",
    "    print('Data batch shape:', data_batch.shape)\n",
    "    print('Labels batch shape:', labels_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37d64f0",
   "metadata": {},
   "source": [
    "*We can appreciate that...*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74724be9",
   "metadata": {},
   "source": [
    "## 5.1.3. Training\n",
    "\n",
    "Let's train the model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7858981a",
   "metadata": {},
   "source": [
    "We use [Early Stopping](https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/) to avoid *overfitting*, as well `ModelCheckpoint` to save the best model obtained during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4053f1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model name and path\n",
    "model_path = os.path.join(\"models\", \"vgg19_frozen_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75165a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
    "mc = ModelCheckpoint(model_path, monitor='val_loss', \n",
    "                     mode='min', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8cc890",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_features, train_labels,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    epochs=30,\n",
    "    validation_data=(validation_features, validation_labels),\n",
    "    validation_steps=20,\n",
    "    callbacks = [es, mc]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bdfab1",
   "metadata": {},
   "source": [
    "Now let's load the best model found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9948a2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the saved model\n",
    "saved_model = load_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba77e4f",
   "metadata": {},
   "source": [
    "## 5.1.4. Validation\n",
    "\n",
    "Let's plot how the loss and the accuracy from both training and validations sets have evolved during the training process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963577df",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bc513e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss curves\n",
    "plot_metric_curves(epochs, loss, val_loss, \"black\", \"darkgray\", \"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2b49d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy curves\n",
    "plot_metric_curves(epochs, acc, val_acc, \"black\", \"darkgray\", \"Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dd916b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_features, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1ba70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model.evaluate(test_features, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a08e99",
   "metadata": {},
   "source": [
    "*Comments about how those metrics have evolved...*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_art",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
