{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f15229f7-7661-4a51-a3a1-c79c93a54765",
   "metadata": {
    "id": "168YUOMq-SrL"
   },
   "source": [
    "# **5. Xception**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ebd7e6-de01-4dd2-a211-099e327856c2",
   "metadata": {
    "id": "3w6rBQM0_o7T"
   },
   "source": [
    "First of all, let's load the required libraries in order to run the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56148681-04e5-4f5c-8865-a7cfaaea8bca",
   "metadata": {
    "id": "Rh87ovI1-TCv"
   },
   "outputs": [],
   "source": [
    "# Base libraries\n",
    "import os\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Tensorflow\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow import debugging as tfdbg\n",
    "from tensorflow import device\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Xception\n",
    "from tensorflow.keras.applications import Xception\n",
    "\n",
    "# Own modules\n",
    "from src.utils import drop_checkpoints, dataset_stats, plot_metric_curves, extract_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee57621d-c6e3-489f-88ba-d5543ed70490",
   "metadata": {
    "id": "D8x-hAiaAblq"
   },
   "source": [
    "Now let's see how our data is structured:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7558b466-0276-47fa-820e-9b723fa381ea",
   "metadata": {
    "id": "pIfrW7zx-TEX"
   },
   "outputs": [],
   "source": [
    "# Root folder\n",
    "base_dir = \"./data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7622e546-7d82-4a94-a6dd-545a29235884",
   "metadata": {
    "id": "4A14qVrGMomZ"
   },
   "outputs": [],
   "source": [
    "# Train folder\n",
    "train_dir = os.path.join(base_dir, \"train\")\n",
    "\n",
    "# Validation folder\n",
    "validation_dir = os.path.join(base_dir, \"validation\")\n",
    "\n",
    "# Test folder\n",
    "test_dir = os.path.join(base_dir, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebe7c45c-b715-4217-b48b-97343c0f53a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_checkpoints(base_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8258082b-1c56-445c-a355-13a882a48c6a",
   "metadata": {
    "id": "FWvgTz7ejmF0"
   },
   "source": [
    "Let's also see how many images there are for each class in the training, validation and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aaf7d899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 4\n",
      "Existing classes: ['Renaissance', 'Realism', 'Baroque', 'Romanticism']\n",
      "\n",
      "----------------------------------------\n",
      "Number of images per class and dataset:\n",
      "----------------------------------------\n",
      "             Train  Validation  Test\n",
      "Style                               \n",
      "Renaissance   4000         500   500\n",
      "Realism       4000         500   500\n",
      "Baroque       4000         500   500\n",
      "Romanticism   4000         500   500\n"
     ]
    }
   ],
   "source": [
    "dataset_stats(train_dir, validation_dir, test_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdd88db-6761-445f-a33e-f60c9bf3e63b",
   "metadata": {},
   "source": [
    "We'll also create the directory, if not created yet, where the models will be saved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f10f504-b5ba-4e94-90d9-f6cc863a1b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory where to save the models created\n",
    "models_dir = \"./models\"\n",
    "os.makedirs(models_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4b50e9-e7b3-41d7-b646-7405621431f2",
   "metadata": {
    "id": "lR3pXpNODl6O",
    "tags": []
   },
   "source": [
    "*Explain dropout and data augmentation, include reference to original paper*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110dead2",
   "metadata": {},
   "source": [
    "# **5.1 Predictions with the convolutional base and train classifier**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d1584f-a8b2-4da6-bd86-f0969351a13e",
   "metadata": {
    "id": "ksRFFq2-WgBw"
   },
   "source": [
    "## 5.1.1. Model structure\n",
    "\n",
    "Let's first create the model structure:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0ad3d9-5b0e-4098-bdda-5a53fd355c69",
   "metadata": {},
   "source": [
    "Firstly, let's define the values of some hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e79ca96-a8b4-47ab-854a-ac04f9da50df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps per epoch: 125\n"
     ]
    }
   ],
   "source": [
    "# Define some hyperparameters' values\n",
    "\n",
    "# Input shape\n",
    "input_shape = (299, 299,  3)\n",
    "\n",
    "# Batch_size and steps per epoch\n",
    "training_size = sum([len(file) for path, folder, file in os.walk(train_dir)])\n",
    "batch_size = 128\n",
    "steps_per_epoch = training_size // batch_size\n",
    "print(f\"Steps per epoch: {steps_per_epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3813804-80a2-421d-b3e0-8f4177051253",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_base = Xception( \n",
    "    include_top=False,\n",
    "    weights=\"imagenet\",\n",
    "    input_shape=input_shape,\n",
    "    pooling=None,\n",
    ")\n",
    "\n",
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5973c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = models.Sequential()\n",
    "\n",
    "# Add the convolutional base\n",
    "# model.add(conv_base)\n",
    "\n",
    "# Add the classifier\n",
    "# model.add(layers.Flatten())\n",
    "# model.add(layers.BatchNormalization())\n",
    "\n",
    "last_layer_shape = (7, 7, 960)\n",
    "last_layer_size = last_layer_shape[0]*last_layer_shape[1]*last_layer_shape[2]\n",
    "\n",
    "model.add(layers.Dense(256, activation='relu', input_dim=last_layer_size))\n",
    "model.add(layers.Dropout(0.25))\n",
    "model.add(layers.BatchNormalization())\n",
    "\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dropout(0.25))\n",
    "\n",
    "# Output layer\n",
    "model.add(layers.Dense(4, activation='softmax'))\n",
    "\n",
    "# Freeze the convolutional base\n",
    "# model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97baed2-fd95-40b0-af7b-64946377cc76",
   "metadata": {
    "id": "R9Juyt3rLCKk"
   },
   "source": [
    "Once the structure of the base model has been defined, let's see exactly how many parameters it has in order to have a better idea of how flexible this model is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7886fd9f-20a3-4e37-9f37-dd0f70b39707",
   "metadata": {
    "id": "Zqn2mE-n-TKl"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07be938-fc08-49cd-9bf3-f2856aa8013b",
   "metadata": {
    "id": "DB3cPq2nLVaR"
   },
   "source": [
    "We'll use Adam as our optimizer since it is the most popular optimizer right now, as well as versatile (i.e., it can be used in multiple contexts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c73ae3a-6296-4aa5-bc43-a9988a33b39c",
   "metadata": {
    "id": "-PG_KXks-TMm"
   },
   "outputs": [],
   "source": [
    "# optimizer = optimizers.SGD(learning_rate=0.1, momentum=0.9, weight_decay=0.0001)\n",
    "\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['acc']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead2b842-77e7-40d5-93c9-40eadf68d664",
   "metadata": {
    "id": "q3I-ZQfNMUej"
   },
   "source": [
    "## 5.1.2. Data preprocessing\n",
    "\n",
    "In this case, we will include the Data Augmentation step to the model preprocessing step..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda7e18a-5f5f-4bc1-b9c9-41c74d986555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply data augmentation to the training set\n",
    "# https://towardsdatascience.com/exploring-image-data-augmentation-with-keras-and-tensorflow-a8162d89b844\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    brightness_range=(0.8, 1),\n",
    "    zoom_range=[0.9, 1.1],\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    "    )\n",
    "\n",
    "# The data augmentation must not be used for the test set!\n",
    "# All images will be rescaled by 1./255\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998fdbe7-8ccf-4408-a334-9f6607d80997",
   "metadata": {
    "id": "rF9cfUPS-TO1"
   },
   "outputs": [],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        # All images will be resized to the dimensions specified\n",
    "        target_size=input_shape[:2],\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=True\n",
    "        )\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        # All images will be resized to the dimensions specified\n",
    "        target_size=input_shape[:2],\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False\n",
    "        )\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        test_dir,\n",
    "        target_size=input_shape[:2],\n",
    "        batch_size=40,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c29a17f-354e-4d74-a2d9-b692b80cfcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, train_labels = extract_features(16000, last_layer_shape, conv_base, train_generator, batch_size)\n",
    "validation_features, validation_labels = extract_features(2000, last_layer_shape, conv_base, validation_generator, batch_size)\n",
    "test_features, test_labels = extract_features(2000, last_layer_shape, conv_base, test_generator, 40)\n",
    "\n",
    "# Flatten features prior to feeding them to the classifier\n",
    "train_features = np.reshape(train_features, (16000, last_layer_size))\n",
    "validation_features = np.reshape(validation_features, (2000, last_layer_size))\n",
    "test_features = np.reshape(test_features, (2000, last_layer_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f44ef6-7ecb-42a1-9a14-6c5c56ba440a",
   "metadata": {
    "id": "3y0iM-VXQovT"
   },
   "source": [
    "Now let's take a look at the output of one of these generators (for instance, the training one):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ca1dc2-110f-4297-b605-2008fda45781",
   "metadata": {
    "id": "1Jf4AvO_-TRW"
   },
   "outputs": [],
   "source": [
    "for data_batch, labels_batch in train_generator:\n",
    "    print('Data batch shape:', data_batch.shape)\n",
    "    print('Labels batch shape:', labels_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300204f0-2752-4a20-aebe-53edd5229816",
   "metadata": {
    "id": "C7W1pa92Q3MK"
   },
   "source": [
    "*We can appreciate that...*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e94a8f7-399d-4d1a-971e-a1e949a3073a",
   "metadata": {
    "id": "ZoO2Da0IR51Y"
   },
   "source": [
    "## 5.1.3. Training\n",
    "\n",
    "Let's train the model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d312a3-85da-493e-8d34-177c9b8436be",
   "metadata": {},
   "source": [
    "We use [Early Stopping](https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/) to avoid *overfitting*, as well `ModelCheckpoint` to save the best model obtained during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cb2bfb-f5a6-420e-b116-08c2b75bed21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model name and path\n",
    "model_path = os.path.join(\"models\", \"xception_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92816f76-414e-421e-9065-9248b20cd79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
    "mc = ModelCheckpoint(model_path, monitor='val_loss', \n",
    "                     mode='min', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a895e76-e3be-4158-b61d-7f6af5b99901",
   "metadata": {
    "id": "wrYxrr1OQ6qH",
    "tags": []
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_features, train_labels,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    epochs=30,\n",
    "    validation_data=(validation_features, validation_labels),\n",
    "    validation_steps=20,\n",
    "    callbacks = [es, mc]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca0f815-2f2f-45b1-93c3-5c4120ac9a66",
   "metadata": {},
   "source": [
    "Now let's load the best model found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2f7580-54fa-441f-8a5a-041e3ba84462",
   "metadata": {
    "id": "R6ScrxYY-TTQ"
   },
   "outputs": [],
   "source": [
    "# load the saved model\n",
    "saved_model = load_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ac2e2c-b0fc-49ed-b23d-f3a2439a71bb",
   "metadata": {
    "id": "JWuATgMUS8bI"
   },
   "source": [
    "## 5.1.4. Validation\n",
    "\n",
    "Let's plot how the loss and the accuracy from both training and validations sets have evolved during the training process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb50f3d-f9db-4fa8-b395-b9c5337dbb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d78281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss curves\n",
    "plot_metric_curves(epochs, loss, val_loss, \"steelblue\", \"slategrey\", \"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a6a5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy curves\n",
    "plot_metric_curves(epochs, acc, val_acc, \"steelblue\", \"slategrey\", \"Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a15d3d-33f0-4f65-871d-bb7048c39494",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_features, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ed2944",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model.evaluate(test_features, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e3f158-c176-4111-b66e-45918c845472",
   "metadata": {
    "id": "pQg4z_l8Tbj9"
   },
   "source": [
    "import numpy as np\n",
    "*Comments about how those metrics have evolved...*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
