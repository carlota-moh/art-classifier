{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f15229f7-7661-4a51-a3a1-c79c93a54765",
   "metadata": {
    "id": "168YUOMq-SrL"
   },
   "source": [
    "# **Art Classifier**\n",
    "\n",
    "## **Non Structured Data**\n",
    "\n",
    "This project has been done by:\n",
    "\n",
    "|Name                    |Email                              |\n",
    "|------------------------|-----------------------------------|\n",
    "|Jorge Ayuso Martínez    |jorgeayusomartinez@alu.comillas.edu|\n",
    "|Carlota Monedero Herranz|carlotamoh@alu.comillas.edu        |\n",
    "|José Manuel Vega Gradit |josemanuel.vega@alu.comillas.edu   |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ebd7e6-de01-4dd2-a211-099e327856c2",
   "metadata": {
    "id": "3w6rBQM0_o7T"
   },
   "source": [
    "First of all, let's load the required libraries in order to run the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56148681-04e5-4f5c-8865-a7cfaaea8bca",
   "metadata": {
    "id": "Rh87ovI1-TCv"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-18 17:04:33.057653: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-03-18 17:04:33.057721: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-03-18 17:04:33.057729: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee57621d-c6e3-489f-88ba-d5543ed70490",
   "metadata": {
    "id": "D8x-hAiaAblq"
   },
   "source": [
    "Now let's see how our data is structured:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7558b466-0276-47fa-820e-9b723fa381ea",
   "metadata": {
    "id": "pIfrW7zx-TEX"
   },
   "outputs": [],
   "source": [
    "# Root folder\n",
    "base_dir = \"./data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7622e546-7d82-4a94-a6dd-545a29235884",
   "metadata": {
    "id": "4A14qVrGMomZ"
   },
   "outputs": [],
   "source": [
    "# Train folder\n",
    "train_dir = os.path.join(base_dir, \"train\")\n",
    "\n",
    "# Validation folder\n",
    "validation_dir = os.path.join(base_dir, \"validation\")\n",
    "\n",
    "# Test folder\n",
    "test_dir = os.path.join(base_dir, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebe7c45c-b715-4217-b48b-97343c0f53a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in os.walk(base_dir):\n",
    "    for folder in path[1]:\n",
    "        if \".ipynb_checkpoints\" in folder:\n",
    "            os.rmdir(os.path.join(path[0], folder))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8258082b-1c56-445c-a355-13a882a48c6a",
   "metadata": {
    "id": "FWvgTz7ejmF0"
   },
   "source": [
    "Let's also see how many images there are for each class in the training, validation and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "906904f6-fe43-4c22-92b9-e68621589e67",
   "metadata": {
    "id": "oGDiubz2jmOT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 4\n",
      "Existing classes:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Renaissance', 'Realism', 'Baroque', 'Romanticism']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of classes\n",
    "n_classes = len(os.listdir(train_dir))\n",
    "print(f\"Number of classes: {n_classes}\")\n",
    "\n",
    "# Get existing classes\n",
    "classes = os.listdir(train_dir)\n",
    "print(\"Existing classes:\\n\")\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a7e6464-a5c6-4dd5-89b1-3ec9601b8a3a",
   "metadata": {
    "id": "fmhnayouqYQl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images per class in Training set:\n",
      "==================================================\n",
      "Renaissance: 4000\n",
      "Realism: 4000\n",
      "Baroque: 4000\n",
      "Romanticism: 4000\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "print(\"Number of images per class in Training set:\")\n",
    "print(\"=\"*50)\n",
    "for cl in classes:\n",
    "    n_images = len(os.listdir(os.path.join(train_dir, cl)))\n",
    "    print(f\"{cl}: {n_images}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9b52947-e881-418a-9309-62c95848cdef",
   "metadata": {
    "id": "_nBKldzsrTm1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images per class in Validation set:\n",
      "==================================================\n",
      "Renaissance: 500\n",
      "Realism: 500\n",
      "Baroque: 500\n",
      "Romanticism: 500\n"
     ]
    }
   ],
   "source": [
    "# Validation\n",
    "print(\"Number of images per class in Validation set:\")\n",
    "print(\"=\"*50)\n",
    "for cl in classes:\n",
    "    n_images = len(os.listdir(os.path.join(validation_dir, cl)))\n",
    "    print(f\"{cl}: {n_images}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "166a82de-e8ac-47c9-98d6-c67987cc805d",
   "metadata": {
    "id": "8aFWVYRKrTzw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images per class in Test set:\n",
      "==================================================\n",
      "Renaissance: 500\n",
      "Realism: 500\n",
      "Baroque: 500\n",
      "Romanticism: 500\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "print(\"Number of images per class in Test set:\")\n",
    "print(\"=\"*50)\n",
    "for cl in classes:\n",
    "    n_images = len(os.listdir(os.path.join(test_dir, cl)))\n",
    "    print(f\"{cl}: {n_images}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdd88db-6761-445f-a33e-f60c9bf3e63b",
   "metadata": {},
   "source": [
    "We'll also create the directory, if not created yet, where the models will be saved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f10f504-b5ba-4e94-90d9-f6cc863a1b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory where to save the models created\n",
    "models_dir = \"./models\"\n",
    "os.makedirs(models_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f6d790",
   "metadata": {},
   "source": [
    "## **2. Dropout model**\n",
    "\n",
    "Our next goal is optimizing the base model so that we can reach higher accuracies and avoid overfitting as much as possible. We will first start by increasing the complexity of the base model (i.e.: adding more layers), as well as incorporating regularization techniques to prevent us from overfitting to the training set. \n",
    "\n",
    "In this notebook we will explore the power of dropout for helping us reduce overfitting to the data. We will start by building a similar architecture to the one used by the base model, reusing the weights from the previously trained model. We will only be training the fully-connected layers in this case, freezing the convolutional part of the network. This will allow for a faster training process (similar to the approach taken in transfer learning), where our only goal is using dropout to avoid overfitting to the data. \n",
    "\n",
    "We will start by defining a relatively small dropout rate (0.05). Since we will be increasing the number of samples by using data augmentation later on, using a higher penalty will probably hinder the training process and ultimately causing the model to meet early stopping criteria before converging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d1584f-a8b2-4da6-bd86-f0969351a13e",
   "metadata": {
    "id": "ksRFFq2-WgBw"
   },
   "source": [
    "### 1.1. Model structure\n",
    "\n",
    "Let's first create the model structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbdf3f8c-271d-4fea-8a4c-e2ad9137b2a5",
   "metadata": {
    "id": "35S_bt4t-TIX"
   },
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "# 1st Convolution Layer\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
    "                        input_shape=(256, 256, 3)\n",
    "                        )\n",
    ")\n",
    "# 1st Pooling Layer\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "# 2nd Convolution Layer\n",
    "model.add(layers.Conv2D(64, \n",
    "                        (3, 3), \n",
    "                        activation='relu'\n",
    "                       )\n",
    "         )\n",
    "# 2nd Pooling Layer\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "# 3rd Convolution Layer\n",
    "model.add(layers.Conv2D(128, \n",
    "                        (3, 3), \n",
    "                        activation='relu'\n",
    "                        )\n",
    ")\n",
    "# 3rd Pooling Layer\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "# 4th Convolution Layer\n",
    "model.add(layers.Conv2D(128, \n",
    "                        (3, 3), \n",
    "                        activation='relu'\n",
    "                        )\n",
    ")\n",
    "# 4th Pooling Layer\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(4, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97baed2-fd95-40b0-af7b-64946377cc76",
   "metadata": {
    "id": "R9Juyt3rLCKk"
   },
   "source": [
    "Once the structure of the base model has been defined, let's see exactly how many parameters it has in order to have a better idea of how flexible this model is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7886fd9f-20a3-4e37-9f37-dd0f70b39707",
   "metadata": {
    "id": "Zqn2mE-n-TKl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 254, 254, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 127, 127, 32)     0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 125, 125, 64)      18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 62, 62, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 60, 60, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 30, 30, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 28, 28, 128)       147584    \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 14, 14, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 25088)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               12845568  \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4)                 2052      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13,088,452\n",
      "Trainable params: 13,088,452\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07be938-fc08-49cd-9bf3-f2856aa8013b",
   "metadata": {
    "id": "DB3cPq2nLVaR"
   },
   "source": [
    "We'll use Adam as our optimizer since it is the most popular optimizer right now, as well as versatile (i.e., it can be used in multiple contexts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c73ae3a-6296-4aa5-bc43-a9988a33b39c",
   "metadata": {
    "id": "-PG_KXks-TMm"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=optimizers.Adam(learning_rate=1e-4),\n",
    "    metrics=['acc']\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ead2b842-77e7-40d5-93c9-40eadf68d664",
   "metadata": {
    "id": "q3I-ZQfNMUej"
   },
   "source": [
    "### 1.2. Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "998fdbe7-8ccf-4408-a334-9f6607d80997",
   "metadata": {
    "id": "rF9cfUPS-TO1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16000 images belonging to 4 classes.\n",
      "Found 2000 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "# All images will be rescaled by 1./255\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        # This is the target directory\n",
    "        train_dir,\n",
    "        # All images will be resized to 150x150\n",
    "        target_size=(256, 256),\n",
    "        batch_size=128,\n",
    "        class_mode='categorical'\n",
    "        )\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size=(256, 256),\n",
    "        batch_size=128,\n",
    "        class_mode='categorical'\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f44ef6-7ecb-42a1-9a14-6c5c56ba440a",
   "metadata": {
    "id": "3y0iM-VXQovT"
   },
   "source": [
    "Now let's take a look at the output of one of these generators (for instance, the training one):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8ca1dc2-110f-4297-b605-2008fda45781",
   "metadata": {
    "id": "1Jf4AvO_-TRW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data batch shape: (128, 256, 256, 3)\n",
      "labels batch shape: (128, 4)\n"
     ]
    }
   ],
   "source": [
    "for data_batch, labels_batch in train_generator:\n",
    "    print('data batch shape:', data_batch.shape)\n",
    "    print('labels batch shape:', labels_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300204f0-2752-4a20-aebe-53edd5229816",
   "metadata": {
    "id": "C7W1pa92Q3MK"
   },
   "source": [
    "*We can appreciate that...*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b0b6e3",
   "metadata": {},
   "source": [
    "### 1.3. Training\n",
    "\n",
    "We can now train this improved version of our model and see if it improves performance upon the first version. We will still implement EarlyStopping to make sure we avoid *overfitting* as much as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2202be1e",
   "metadata": {},
   "source": [
    "We use [Early Stopping](https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/) to limit *overfitting*, as well `ModelCheckpoint` to save the best model obtained during training. We will using validation loss as metric function for early stopping, setting a patience of 5 (i.e.: we will stop after there is no significant change in validation loss for 5 epochs of training). Since we are dealing with a relatively small dataset, we can set a high enough number of epochs (in this case we chose 100), as we can be fairly sure that training will be stopped before reaching the limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92816f76-414e-421e-9065-9248b20cd79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
    "mc = ModelCheckpoint(os.path.join(\"models\", \"dropout_model.h5\"), monitor='val_loss', \n",
    "                     mode='min', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a895e76-e3be-4158-b61d-7f6af5b99901",
   "metadata": {
    "id": "wrYxrr1OQ6qH",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " 51/125 [===========>..................] - ETA: 2:54 - loss: 1.3105 - acc: 0.3775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/PIL/TiffImagePlugin.py:858: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - ETA: 0s - loss: 1.2409 - acc: 0.4348\n",
      "Epoch 1: val_loss improved from inf to 1.14140, saving model to models/dropout_model.h5\n",
      "125/125 [==============================] - 337s 3s/step - loss: 1.2409 - acc: 0.4348 - val_loss: 1.1414 - val_acc: 0.5271\n",
      "Epoch 2/100\n",
      "125/125 [==============================] - ETA: 0s - loss: 1.0943 - acc: 0.5332\n",
      "Epoch 2: val_loss improved from 1.14140 to 1.06377, saving model to models/dropout_model.h5\n",
      "125/125 [==============================] - 325s 3s/step - loss: 1.0943 - acc: 0.5332 - val_loss: 1.0638 - val_acc: 0.5578\n",
      "Epoch 3/100\n",
      "125/125 [==============================] - ETA: 0s - loss: 1.0439 - acc: 0.5589\n",
      "Epoch 3: val_loss improved from 1.06377 to 1.04728, saving model to models/dropout_model.h5\n",
      "125/125 [==============================] - 323s 3s/step - loss: 1.0439 - acc: 0.5589 - val_loss: 1.0473 - val_acc: 0.5667\n",
      "Epoch 4/100\n",
      "125/125 [==============================] - ETA: 0s - loss: 1.0066 - acc: 0.5754\n",
      "Epoch 4: val_loss improved from 1.04728 to 1.01971, saving model to models/dropout_model.h5\n",
      "125/125 [==============================] - 323s 3s/step - loss: 1.0066 - acc: 0.5754 - val_loss: 1.0197 - val_acc: 0.5786\n",
      "Epoch 5/100\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.9748 - acc: 0.5917\n",
      "Epoch 5: val_loss improved from 1.01971 to 0.99187, saving model to models/dropout_model.h5\n",
      "125/125 [==============================] - 323s 3s/step - loss: 0.9748 - acc: 0.5917 - val_loss: 0.9919 - val_acc: 0.5964\n",
      "Epoch 6/100\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.9326 - acc: 0.6138\n",
      "Epoch 6: val_loss improved from 0.99187 to 0.97422, saving model to models/dropout_model.h5\n",
      "125/125 [==============================] - 323s 3s/step - loss: 0.9326 - acc: 0.6138 - val_loss: 0.9742 - val_acc: 0.6021\n",
      "Epoch 7/100\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.9086 - acc: 0.6227\n",
      "Epoch 7: val_loss improved from 0.97422 to 0.96687, saving model to models/dropout_model.h5\n",
      "125/125 [==============================] - 323s 3s/step - loss: 0.9086 - acc: 0.6227 - val_loss: 0.9669 - val_acc: 0.6172\n",
      "Epoch 8/100\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.8859 - acc: 0.6364\n",
      "Epoch 8: val_loss improved from 0.96687 to 0.95167, saving model to models/dropout_model.h5\n",
      "125/125 [==============================] - 323s 3s/step - loss: 0.8859 - acc: 0.6364 - val_loss: 0.9517 - val_acc: 0.6198\n",
      "Epoch 9/100\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.8501 - acc: 0.6510\n",
      "Epoch 9: val_loss did not improve from 0.95167\n",
      "125/125 [==============================] - 323s 3s/step - loss: 0.8501 - acc: 0.6510 - val_loss: 0.9682 - val_acc: 0.6187\n",
      "Epoch 10/100\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.8232 - acc: 0.6656\n",
      "Epoch 10: val_loss did not improve from 0.95167\n",
      "125/125 [==============================] - 321s 3s/step - loss: 0.8232 - acc: 0.6656 - val_loss: 0.9584 - val_acc: 0.6182\n",
      "Epoch 11/100\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.8063 - acc: 0.6743\n",
      "Epoch 11: val_loss improved from 0.95167 to 0.93849, saving model to models/dropout_model.h5\n",
      "125/125 [==============================] - 324s 3s/step - loss: 0.8063 - acc: 0.6743 - val_loss: 0.9385 - val_acc: 0.6271\n",
      "Epoch 12/100\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.7703 - acc: 0.6911\n",
      "Epoch 12: val_loss did not improve from 0.93849\n",
      "125/125 [==============================] - 323s 3s/step - loss: 0.7703 - acc: 0.6911 - val_loss: 0.9395 - val_acc: 0.6328\n",
      "Epoch 13/100\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.7477 - acc: 0.6993\n",
      "Epoch 13: val_loss did not improve from 0.93849\n",
      "125/125 [==============================] - 321s 3s/step - loss: 0.7477 - acc: 0.6993 - val_loss: 0.9707 - val_acc: 0.6281\n",
      "Epoch 14/100\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.7128 - acc: 0.7143\n",
      "Epoch 14: val_loss did not improve from 0.93849\n",
      "125/125 [==============================] - 323s 3s/step - loss: 0.7128 - acc: 0.7143 - val_loss: 0.9541 - val_acc: 0.6302\n",
      "Epoch 15/100\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.6770 - acc: 0.7355\n",
      "Epoch 15: val_loss did not improve from 0.93849\n",
      "125/125 [==============================] - 322s 3s/step - loss: 0.6770 - acc: 0.7355 - val_loss: 0.9440 - val_acc: 0.6323\n",
      "Epoch 16/100\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.6579 - acc: 0.7442\n",
      "Epoch 16: val_loss improved from 0.93849 to 0.93692, saving model to models/dropout_model.h5\n",
      "125/125 [==============================] - 322s 3s/step - loss: 0.6579 - acc: 0.7442 - val_loss: 0.9369 - val_acc: 0.6453\n",
      "Epoch 17/100\n",
      " 60/125 [=============>................] - ETA: 2:22 - loss: 0.6058 - acc: 0.7682"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=125,\n",
    "    epochs=100,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=15,\n",
    "    callbacks = [es, mc]\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca0f815-2f2f-45b1-93c3-5c4120ac9a66",
   "metadata": {},
   "source": [
    "Now let's load the best model found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2f7580-54fa-441f-8a5a-041e3ba84462",
   "metadata": {
    "id": "R6ScrxYY-TTQ"
   },
   "outputs": [],
   "source": [
    "# load the saved model\n",
    "base_model = load_model(os.path.join(\"models\", \"dropout_model.h5\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ac2e2c-b0fc-49ed-b23d-f3a2439a71bb",
   "metadata": {
    "id": "JWuATgMUS8bI"
   },
   "source": [
    "### 1.4. Validation\n",
    "\n",
    "Let's plot how the loss and the accuracy from both training and validations sets have evolved during the training process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb50f3d-f9db-4fa8-b395-b9c5337dbb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2eef723-37b8-452f-9675-2b640325e316",
   "metadata": {
    "id": "kXvSCIRP-TVZ"
   },
   "outputs": [],
   "source": [
    "sns.set_theme()\n",
    "\n",
    "plt.figure(figsize=(15,10), dpi=200)\n",
    "\n",
    "plt.plot(epochs, acc, 'royalblue', linewidth=2, label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'blueviolet', linewidth=2, label='Validation acc')\n",
    "plt.title('Training and validation accuracy', fontsize=20)\n",
    "plt.legend(frameon=False, fontsize=15)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(15,10), dpi=200)\n",
    "\n",
    "plt.plot(epochs, loss, 'royalblue', linewidth=2, label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'blueviolet', linewidth=2, label='Validation loss')\n",
    "plt.title('Training and validation loss', fontsize=20)\n",
    "plt.legend(frameon=False, fontsize=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d735f4-0de1-4d11-9761-bb812d5cbcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator = test_datagen.flow_from_directory(\n",
    "        test_dir,\n",
    "        target_size=(256, 256),\n",
    "        batch_size=128,\n",
    "        class_mode='categorical'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a15d3d-33f0-4f65-871d-bb7048c39494",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b59d53",
   "metadata": {},
   "source": [
    "As we can see, the level of dropout implemented was not enough to prevent overfitting from the model and only helped us marginally increase performance in the validation dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
