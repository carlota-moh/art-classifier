{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f15229f7-7661-4a51-a3a1-c79c93a54765",
   "metadata": {
    "id": "168YUOMq-SrL"
   },
   "source": [
    "# **5. ResNet-50**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ebd7e6-de01-4dd2-a211-099e327856c2",
   "metadata": {
    "id": "3w6rBQM0_o7T"
   },
   "source": [
    "First of all, let's load the required libraries in order to run the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56148681-04e5-4f5c-8865-a7cfaaea8bca",
   "metadata": {
    "id": "Rh87ovI1-TCv"
   },
   "outputs": [],
   "source": [
    "# Base libraries\n",
    "import os\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Tensorflow\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# ResNet50\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "\n",
    "# Own modules\n",
    "from src.utils import drop_checkpoints, dataset_stats, plot_metric_curves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee57621d-c6e3-489f-88ba-d5543ed70490",
   "metadata": {
    "id": "D8x-hAiaAblq"
   },
   "source": [
    "Now let's see how our data is structured:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7558b466-0276-47fa-820e-9b723fa381ea",
   "metadata": {
    "id": "pIfrW7zx-TEX"
   },
   "outputs": [],
   "source": [
    "# Root folder\n",
    "base_dir = \"./data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7622e546-7d82-4a94-a6dd-545a29235884",
   "metadata": {
    "id": "4A14qVrGMomZ"
   },
   "outputs": [],
   "source": [
    "# Train folder\n",
    "train_dir = os.path.join(base_dir, \"train\")\n",
    "\n",
    "# Validation folder\n",
    "validation_dir = os.path.join(base_dir, \"validation\")\n",
    "\n",
    "# Test folder\n",
    "test_dir = os.path.join(base_dir, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebe7c45c-b715-4217-b48b-97343c0f53a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_checkpoints(base_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8258082b-1c56-445c-a355-13a882a48c6a",
   "metadata": {
    "id": "FWvgTz7ejmF0"
   },
   "source": [
    "Let's also see how many images there are for each class in the training, validation and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aaf7d899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 4\n",
      "Existing classes: ['Baroque', 'Realism', 'Renaissance', 'Romanticism']\n",
      "\n",
      "----------------------------------------\n",
      "Number of images per class and dataset:\n",
      "----------------------------------------\n",
      "             Train  Validation  Test\n",
      "Style                               \n",
      "Baroque       4000         500   500\n",
      "Realism       4000         500   500\n",
      "Renaissance   4000         500   500\n",
      "Romanticism   4000         500   500\n"
     ]
    }
   ],
   "source": [
    "dataset_stats(train_dir, validation_dir, test_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdd88db-6761-445f-a33e-f60c9bf3e63b",
   "metadata": {},
   "source": [
    "We'll also create the directory, if not created yet, where the models will be saved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f10f504-b5ba-4e94-90d9-f6cc863a1b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory where to save the models created\n",
    "models_dir = \"./models\"\n",
    "os.makedirs(models_dir, exist_ok=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fe4b50e9-e7b3-41d7-b646-7405621431f2",
   "metadata": {
    "id": "lR3pXpNODl6O",
    "tags": []
   },
   "source": [
    "*Explain dropout and data augmentation, include reference to original paper*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "27d1584f-a8b2-4da6-bd86-f0969351a13e",
   "metadata": {
    "id": "ksRFFq2-WgBw"
   },
   "source": [
    "### 3.1. Model structure\n",
    "\n",
    "Let's first create the model structure:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0ad3d9-5b0e-4098-bdda-5a53fd355c69",
   "metadata": {},
   "source": [
    "Firstly, let's define the values of some hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e79ca96-a8b4-47ab-854a-ac04f9da50df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps per epoch: 125\n"
     ]
    }
   ],
   "source": [
    "# Define some hyperparameters' values\n",
    "\n",
    "# Input shape\n",
    "input_shape = (256, 256,  3)\n",
    "\n",
    "# Batch_size and steps per epoch\n",
    "training_size = sum([len(file) for path, folder, file in os.walk(train_dir)])\n",
    "batch_size = 128\n",
    "steps_per_epoch = training_size // batch_size\n",
    "print(f\"Steps per epoch: {steps_per_epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b5973c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RestNet50 model with imagenet weights and without the classifier, just the convolutional base\n",
    "# The classifier will be added later by us to adapt it to our problem\n",
    "conv_base = ResNet50( \n",
    "    include_top=False,\n",
    "    weights=\"imagenet\",\n",
    "    pooling=\"max\",\n",
    "    input_shape=input_shape\n",
    "    )\n",
    "\n",
    "# Freeze the convolutional base\n",
    "conv_base.trainable = False\n",
    "\n",
    "# Create the model\n",
    "model = models.Sequential()\n",
    "\n",
    "# Add the convolutional base\n",
    "model.add(conv_base)\n",
    "\n",
    "# Add the classifier\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "\n",
    "# Output layer\n",
    "model.add(layers.Dense(4, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97baed2-fd95-40b0-af7b-64946377cc76",
   "metadata": {
    "id": "R9Juyt3rLCKk"
   },
   "source": [
    "Once the structure of the base model has been defined, let's see exactly how many parameters it has in order to have a better idea of how flexible this model is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7886fd9f-20a3-4e37-9f37-dd0f70b39707",
   "metadata": {
    "id": "Zqn2mE-n-TKl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " resnet50 (Functional)       (None, 2048)              23587712  \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 2048)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               1049088   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4)                 2052      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 24,638,852\n",
      "Trainable params: 1,051,140\n",
      "Non-trainable params: 23,587,712\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07be938-fc08-49cd-9bf3-f2856aa8013b",
   "metadata": {
    "id": "DB3cPq2nLVaR"
   },
   "source": [
    "We'll use Adam as our optimizer since it is the most popular optimizer right now, as well as versatile (i.e., it can be used in multiple contexts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c73ae3a-6296-4aa5-bc43-a9988a33b39c",
   "metadata": {
    "id": "-PG_KXks-TMm"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=optimizers.Adam(),\n",
    "    metrics=['acc']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead2b842-77e7-40d5-93c9-40eadf68d664",
   "metadata": {
    "id": "q3I-ZQfNMUej"
   },
   "source": [
    "### 1.2. Data preprocessing\n",
    "\n",
    "In this case, we will include the Data Augmentation step to the model preprocessing step..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "998fdbe7-8ccf-4408-a334-9f6607d80997",
   "metadata": {
    "id": "rF9cfUPS-TO1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16000 images belonging to 4 classes.\n",
      "Found 2000 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "# Apply data augmentation to the training set\n",
    "# https://towardsdatascience.com/exploring-image-data-augmentation-with-keras-and-tensorflow-a8162d89b844\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=5,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    brightness_range=(0.9, 1),\n",
    "    zoom_range=[0.95, 1.05],\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    "    )\n",
    "\n",
    "# The data augmentation must not be used for the test set!\n",
    "# All images will be rescaled by 1./255\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        # All images will be resized to the dimensions specified\n",
    "        target_size=input_shape[:2],\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=True\n",
    "        )\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        # All images will be resized to the dimensions specified\n",
    "        target_size=input_shape[:2],\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f44ef6-7ecb-42a1-9a14-6c5c56ba440a",
   "metadata": {
    "id": "3y0iM-VXQovT"
   },
   "source": [
    "Now let's take a look at the output of one of these generators (for instance, the training one):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8ca1dc2-110f-4297-b605-2008fda45781",
   "metadata": {
    "id": "1Jf4AvO_-TRW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data batch shape: (128, 256, 256, 3)\n",
      "Labels batch shape: (128, 4)\n"
     ]
    }
   ],
   "source": [
    "for data_batch, labels_batch in train_generator:\n",
    "    print('Data batch shape:', data_batch.shape)\n",
    "    print('Labels batch shape:', labels_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300204f0-2752-4a20-aebe-53edd5229816",
   "metadata": {
    "id": "C7W1pa92Q3MK"
   },
   "source": [
    "*We can appreciate that...*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e94a8f7-399d-4d1a-971e-a1e949a3073a",
   "metadata": {
    "id": "ZoO2Da0IR51Y"
   },
   "source": [
    "### 1.3. Training\n",
    "\n",
    "Let's train the model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d312a3-85da-493e-8d34-177c9b8436be",
   "metadata": {},
   "source": [
    "We use [Early Stopping](https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/) to avoid *overfitting*, as well `ModelCheckpoint` to save the best model obtained during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0cb2bfb-f5a6-420e-b116-08c2b75bed21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model name and path\n",
    "model_path = os.path.join(\"models\", \"resnet50_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92816f76-414e-421e-9065-9248b20cd79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
    "mc = ModelCheckpoint(model_path, monitor='val_loss', \n",
    "                     mode='min', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a895e76-e3be-4158-b61d-7f6af5b99901",
   "metadata": {
    "id": "wrYxrr1OQ6qH",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      " 31/125 [======>.......................] - ETA: 18:45 - loss: 3.0518 - acc: 0.2571"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m      2\u001b[0m     train_generator,\n\u001b[0;32m      3\u001b[0m     steps_per_epoch\u001b[39m=\u001b[39;49msteps_per_epoch,\n\u001b[0;32m      4\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m30\u001b[39;49m,\n\u001b[0;32m      5\u001b[0m     validation_data\u001b[39m=\u001b[39;49mvalidation_generator,\n\u001b[0;32m      6\u001b[0m     validation_steps\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m,\n\u001b[0;32m      7\u001b[0m     callbacks \u001b[39m=\u001b[39;49m [es, mc]\n\u001b[0;32m      8\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\jorge\\miniconda3\\envs\\dl\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\jorge\\miniconda3\\envs\\dl\\lib\\site-packages\\keras\\engine\\training.py:1650\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1642\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1643\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1644\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1648\u001b[0m ):\n\u001b[0;32m   1649\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1650\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1651\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1652\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\jorge\\miniconda3\\envs\\dl\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\jorge\\miniconda3\\envs\\dl\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:880\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    877\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    879\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 880\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    882\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    883\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\jorge\\miniconda3\\envs\\dl\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:912\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    909\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    910\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    911\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 912\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_no_variable_creation_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    913\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    914\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    915\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    916\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\jorge\\miniconda3\\envs\\dl\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:134\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    132\u001b[0m   (concrete_function,\n\u001b[0;32m    133\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 134\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m    135\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\jorge\\miniconda3\\envs\\dl\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1745\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1741\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1742\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1743\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1744\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1745\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1746\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1747\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1748\u001b[0m     args,\n\u001b[0;32m   1749\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1750\u001b[0m     executing_eagerly)\n\u001b[0;32m   1751\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\jorge\\miniconda3\\envs\\dl\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:378\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    377\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 378\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    379\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    380\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    381\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    382\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    383\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    384\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    385\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    386\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    387\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    390\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    391\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\jorge\\miniconda3\\envs\\dl\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    epochs=30,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=10,\n",
    "    callbacks = [es, mc]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca0f815-2f2f-45b1-93c3-5c4120ac9a66",
   "metadata": {},
   "source": [
    "Now let's load the best model found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb2f7580-54fa-441f-8a5a-041e3ba84462",
   "metadata": {
    "id": "R6ScrxYY-TTQ"
   },
   "outputs": [],
   "source": [
    "# load the saved model\n",
    "saved_model = load_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ac2e2c-b0fc-49ed-b23d-f3a2439a71bb",
   "metadata": {
    "id": "JWuATgMUS8bI"
   },
   "source": [
    "### 1.4. Validation\n",
    "\n",
    "Let's plot how the loss and the accuracy from both training and validations sets have evolved during the training process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3bb50f3d-f9db-4fa8-b395-b9c5337dbb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d78281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss curves\n",
    "plot_metric_curves(epochs, loss, val_loss, \"darkorange\", \"navajowhite\", \"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a6a5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy curves\n",
    "plot_metric_curves(epochs, acc, val_acc, \"darkorange\", \"navajowhite\", \"Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "91d735f4-0de1-4d11-9761-bb812d5cbcb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "test_generator = test_datagen.flow_from_directory(\n",
    "        test_dir,\n",
    "        target_size=input_shape[:2],\n",
    "        batch_size=40,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "95a15d3d-33f0-4f65-871d-bb7048c39494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 47s 943ms/step - loss: 1.3300 - acc: 0.5755\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.3299739360809326, 0.5755000114440918]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "68ed2944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 44s 876ms/step - loss: 1.2042 - acc: 0.5385\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.2042485475540161, 0.5385000109672546]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_model.evaluate(test_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e3f158-c176-4111-b66e-45918c845472",
   "metadata": {
    "id": "pQg4z_l8Tbj9"
   },
   "source": [
    "*Comments about how those metrics have evolved...*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
